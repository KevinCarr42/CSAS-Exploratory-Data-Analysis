{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sept 24 Coordinators Meeting - Text Extraction & Analysis\n",
    "Processing meeting documents through extraction, cleaning, categorization, and summarization."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T14:30:01.172324Z",
     "start_time": "2025-11-19T14:30:01.163961Z"
    }
   },
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Setup\n",
    "meeting_folder = \"sept_24_coordinators_meeting\"\n",
    "data_dir = os.path.join(meeting_folder, 'data')\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# Import helper functions\n",
    "sys.path.insert(0, os.getcwd())\n",
    "from extract_helpers import extract_docx_content, extract_pptx_content\n",
    "from categorize_helpers import detect_language, normalize_text, categorize_content, detect_themes, THEMES\n",
    "from summary_helpers import export_summaries, export_theme_analysis\n",
    "from analysis_helpers import (analyze_patterns, identify_conflicts, identify_high_priority_items,\n",
    "                               generate_stakeholder_summary, generate_follow_up_actions, export_analysis)\n",
    "\n",
    "print(\"Setup complete. Ready to process meeting documents.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete. Ready to process meeting documents.\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2-3: Extract and Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T14:30:01.906058Z",
     "start_time": "2025-11-19T14:30:01.231047Z"
    }
   },
   "source": [
    "# Extract all documents\n",
    "all_data = []\n",
    "for file_name in sorted(os.listdir(meeting_folder)):\n",
    "    file_path = os.path.join(meeting_folder, file_name)\n",
    "    if not os.path.isfile(file_path):\n",
    "        continue\n",
    "\n",
    "    file_ext = Path(file_path).suffix.lower()\n",
    "    content = []\n",
    "\n",
    "    if file_ext == '.docx':\n",
    "        content = extract_docx_content(file_path)\n",
    "    elif file_ext == '.pptx':\n",
    "        content = extract_pptx_content(file_path)\n",
    "\n",
    "    for item in content:\n",
    "        all_data.append({\n",
    "            'source_file': file_name,\n",
    "            'source_type': file_ext,\n",
    "            'text': item['text'],\n",
    "            'element_type': item['element_type'],\n",
    "            'style': item['style']\n",
    "        })\n",
    "\n",
    "df_raw = pd.DataFrame(all_data)\n",
    "df_raw.insert(0, 'row_id', range(1, len(df_raw) + 1))\n",
    "\n",
    "print(f\"Extracted {len(df_raw)} rows from {df_raw['source_file'].nunique()} documents\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 862 rows from 12 documents\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 3-4: Categorize and Refine"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T14:30:04.023396Z",
     "start_time": "2025-11-19T14:30:01.991227Z"
    }
   },
   "source": [
    "# Add language detection and categorization\n",
    "df_clean = df_raw.copy()\n",
    "df_clean['language'] = df_clean['text'].apply(detect_language)\n",
    "df_clean['text_normalized'] = df_clean['text'].apply(normalize_text)\n",
    "\n",
    "# Categorize content\n",
    "categorization = df_clean['text'].apply(categorize_content)\n",
    "df_clean['action_categories'] = categorization.apply(lambda x: x['action_categories'])\n",
    "df_clean['recommendation_categories'] = categorization.apply(lambda x: x['recommendation_categories'])\n",
    "df_clean['contention_categories'] = categorization.apply(lambda x: x['contention_categories'])\n",
    "\n",
    "# Add strength scores\n",
    "df_clean['action_strength'] = df_clean['action_categories'].apply(len)\n",
    "df_clean['recommendation_strength'] = df_clean['recommendation_categories'].apply(len)\n",
    "df_clean['contention_strength'] = df_clean['contention_categories'].apply(len)\n",
    "\n",
    "# Mark document status\n",
    "df_clean['document_status'] = 'primary'\n",
    "df_clean.loc[df_clean['source_file'] == 'F2F Meeting Report (near final).docx', 'document_status'] = 'superseded'\n",
    "df_clean.loc[df_clean['source_file'].isin([\n",
    "    'CSAS Transformation update.pptx', \n",
    "    'CSAS Transformation update-FR.pptx'\n",
    "]), 'document_status'] = 'primary_translated'\n",
    "\n",
    "# Create primary dataset (exclude superseded)\n",
    "df_primary = df_clean[df_clean['document_status'] != 'superseded'].copy()\n",
    "\n",
    "# Export processed data\n",
    "df_clean.to_pickle(os.path.join(data_dir, 'meeting_data_refined.pkl'))\n",
    "df_primary.to_pickle(os.path.join(data_dir, 'meeting_data_primary.pkl'))\n",
    "\n",
    "print(f\"Processed: {len(df_clean)} total rows, {len(df_primary)} primary rows\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: 862 total rows, 689 primary rows\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 5: Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T14:30:04.112999Z",
     "start_time": "2025-11-19T14:30:04.106860Z"
    }
   },
   "source": [
    "# Quick quality check\n",
    "qa_metrics = {\n",
    "    'total_rows': len(df_primary),\n",
    "    'documents': df_primary['source_file'].nunique(),\n",
    "    'languages': df_primary['language'].nunique(),\n",
    "    'action_items': (df_primary['action_strength'] > 0).sum(),\n",
    "    'recommendations': (df_primary['recommendation_strength'] > 0).sum(),\n",
    "    'contentions': (df_primary['contention_strength'] > 0).sum(),\n",
    "    'null_values': df_primary.isnull().sum().sum(),\n",
    "    'language_detection_rate': f\"{(df_primary['language'] != 'unknown').sum() / len(df_primary) * 100:.1f}%\"\n",
    "}\n",
    "\n",
    "print(\"Quality Assessment:\")\n",
    "for key, value in qa_metrics.items():\n",
    "    print(f\"  {key}: {value}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quality Assessment:\n",
      "  total_rows: 689\n",
      "  documents: 11\n",
      "  languages: 20\n",
      "  action_items: 86\n",
      "  recommendations: 130\n",
      "  contentions: 91\n",
      "  null_values: 0\n",
      "  language_detection_rate: 95.6%\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 6: Final Summarization"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T14:30:04.243505Z",
     "start_time": "2025-11-19T14:30:04.161839Z"
    }
   },
   "source": [
    "# Load primary dataset and add themes\n",
    "df_summary = pd.read_pickle(os.path.join(data_dir, 'meeting_data_primary.pkl'))\n",
    "df_summary['themes'] = df_summary['text'].apply(detect_themes)\n",
    "\n",
    "# Export summaries\n",
    "action_summary, recommendations_summary, contentions_summary = export_summaries(df_summary, data_dir)\n",
    "theme_counts = export_theme_analysis(df_summary, THEMES, data_dir)\n",
    "\n",
    "print(f\"Summary exported:\")\n",
    "print(f\"  Action items: {action_summary['total_action_items']}\")\n",
    "print(f\"  Recommendations: {recommendations_summary['total_recommendations']}\")\n",
    "print(f\"  Contentions: {contentions_summary['total_contentions']}\")\n",
    "print(f\"  Themes: {len(theme_counts)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary exported:\n",
      "  Action items: 17\n",
      "  Recommendations: 19\n",
      "  Contentions: 2\n",
      "  Themes: 6\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 7: Analysis & Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T14:33:46.043229Z",
     "start_time": "2025-11-19T14:33:45.797531Z"
    }
   },
   "source": [
    "# Load summary data\n",
    "df_analysis = pd.read_pickle(os.path.join(data_dir, 'meeting_data_summary.pkl'))\n",
    "\n",
    "# Run analysis\n",
    "patterns = analyze_patterns(df_analysis)\n",
    "conflicts = identify_conflicts(df_analysis)\n",
    "high_priority = identify_high_priority_items(df_analysis)\n",
    "stakeholder_summary = generate_stakeholder_summary(df_analysis)\n",
    "follow_ups = generate_follow_up_actions(df_analysis)\n",
    "\n",
    "# Export all analysis\n",
    "exported_files = export_analysis(patterns, conflicts, high_priority, stakeholder_summary, follow_ups, data_dir)\n",
    "\n",
    "print(\"Phase 7 Analysis Complete:\")\n",
    "print(f\"  Conflicts identified: {len(conflicts)}\")\n",
    "print(f\"  High priority items: {len(high_priority['action_items']) + len(high_priority['recommendations']) + len(high_priority['contentions'])}\")\n",
    "print(f\"  Themes with summaries: {len(stakeholder_summary)}\")\n",
    "print(f\"  Follow-up actions: {len(follow_ups)}\")\n",
    "print(f\"\\nExported files:\")\n",
    "for key, value in exported_files.items():\n",
    "    print(f\"  {value}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 7 Analysis Complete:\n",
      "  Conflicts identified: 0\n",
      "  High priority items: 5\n",
      "  Themes with summaries: 6\n",
      "  Follow-up actions: 103\n",
      "\n",
      "Exported files:\n",
      "  phase7_patterns.json\n",
      "  phase7_conflicts.json\n",
      "  phase7_high_priority.json\n",
      "  phase7_stakeholder_summary.json\n",
      "  phase7_follow_up_actions.json\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
